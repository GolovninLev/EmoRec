{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T14:56:20.754190Z","iopub.status.busy":"2024-04-13T14:56:20.753816Z","iopub.status.idle":"2024-04-13T14:56:57.298685Z","shell.execute_reply":"2024-04-13T14:56:57.297773Z","shell.execute_reply.started":"2024-04-13T14:56:20.754155Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","\n","wandb.login()"]},{"cell_type":"code","execution_count":2,"metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2024-04-13T14:56:57.301514Z","iopub.status.busy":"2024-04-13T14:56:57.300640Z","iopub.status.idle":"2024-04-13T14:57:02.152343Z","shell.execute_reply":"2024-04-13T14:57:02.151393Z","shell.execute_reply.started":"2024-04-13T14:56:57.301477Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import os\n","import platform\n","import random\n","import datetime\n","import joblib\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","import numpy as np\n","\n","import timm\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader, random_split\n","\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.metrics import average_precision_score, cohen_kappa_score, matthews_corrcoef, precision_recall_fscore_support\n","\n","\n","random.seed(0)\n","np.random.seed(0)\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T14:57:02.154001Z","iopub.status.busy":"2024-04-13T14:57:02.153551Z","iopub.status.idle":"2024-04-13T14:57:02.165605Z","shell.execute_reply":"2024-04-13T14:57:02.164645Z","shell.execute_reply.started":"2024-04-13T14:57:02.153975Z"},"trusted":true},"outputs":[],"source":["dt = datetime.datetime.now().strftime(\"%m.%d_%H-%M-%S\")\n","wandb_project_name = \"emo_rec\" # \"emo_rec\" dirty\n","experiment_name = f'{dt}' # model_file_name\n","run_desc=f\"\"\n","\n","torch.backends.cudnn.deterministic = True\n","\n","\n","folder_with_data = 'train-5000-190-14000resol'\n","# folder_with_data = '35887-48x48-fer-2013'\n","# folder_with_data = 'train-5000-190-35000-14000resol'\n","# folder_with_data = 'train-5000-190'\n","# folder_with_data = '35887-48x48-emotion-detection'\n","# folder_with_data = 'debug'\n","\n","model_name = 'vgg19'\n","# model_name = 'resnet50'\n","# model_name = 'vgg_face_dag'\n","# model_name = 'seresnet50__ra2_in1k' # жёстко переобучился\n","# model_name = 'seresnet50__a1_in1k' # круто медленно учится\n","# model_name = 'seresnextaa101d_32x8d__sw_in12k_ft_in1k' \n","# model_name = 'resnext101_32x32d__fb_wsl_ig1b_ft_in1k' # 15.7/16 Gb GPU mem при bs=15\n","# model_name = 'vit_large_patch14_clip_224.openai_ft_in12k_in1k'\n","# model_name = 'resnext101_32x4d.fb_swsl_ig1b_ft_in1k'\n","# model_name = 'resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k'\n","# model_name = 'seresnext101_32x4d.gluon_in1k'\n","\n","\n","batch_size = 1 # 100 25 15 12 6 1 ########### 15 15 15 15 15 (next на 14 или 12 вроде качество лучше) \n","lr = 0.000225 # 00025 00015 000175 000225\n"," #  0.001\n","\n","optim_name = 'SGD' # SGD  Adam  NAdam    Adamax > RAdam\n","# betas=(0.9, 0.999)\n","# eps=1e-08\n","momentum = 0.9\n","\n","\n","start_epoch = 0 # !!!\n","num_epochs = 10 # 40\n","\n","min_acc_to_save = 0.745\n","max_loss_to_save = 1.75\n","\n","\n","num_classes = 8 # !!!\n","\n","path_to_pr_model = r''\n","# path_to_pr_model = r'/kaggle/input/k01-31-12-56-04/k01.31 12-56-04.pth'\n","# path_to_pr_model = r'/kaggle/input/vgg-face-dag/vgg_face_dag.pth'\n","\n","\n","\n","data_transforms = {\n","    folder_with_data: transforms.Compose([\n","        transforms.RandomHorizontalFlip(), # Случайное горизонтальное отражение изображения (зеркальное отражение).\n","        transforms.RandomRotation(degrees=10), # Случайный поворот изображения на заданный угол в градусах.\n","        transforms.ColorJitter(brightness=0.30, contrast=0.30), # Случайное изменение яркости и контраста изображения.\n","        transforms.RandomResizedCrop((224, 224), scale=(0.8, 1.0)),\n","        transforms.RandomApply([transforms.GaussianBlur(3)], p=0.25), # Добавление размытия\n","\n","        transforms.Resize((224, 224)), # Измените размер изображения на 224x224\n","        transforms.ToTensor(), # Преобразуйте изображение в тензор\n","        transforms.Normalize(\n","            mean=[0.485, 0.456, 0.406], # Нормализация по средним значениям и стандартным отклонениям ImageNet\n","            std=[0.229, 0.224, 0.225]\n","        )\n","    ]),\n","}\n","\n","\n","\n","\n","target_names = [\"anger\", \"contempt\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"] # , \"uncertain\", ] # !!\n","\n","train_test_split = 0.25\n","\n","wandb_local_dir = r\"../\"                                         "]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T14:57:02.168240Z","iopub.status.busy":"2024-04-13T14:57:02.167929Z","iopub.status.idle":"2024-04-13T14:57:02.199247Z","shell.execute_reply":"2024-04-13T14:57:02.198332Z","shell.execute_reply.started":"2024-04-13T14:57:02.168215Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class vgg_face_dag(nn.Module):\n","\n","    def __init__(self):\n","        super(vgg_face_dag, self).__init__()\n","        \n","        self.meta = {'mean': [129.186279296875, 104.76238250732422, 93.59396362304688],\n","                     'std': [1, 1, 1],\n","                     'imageSize': [224, 224, 3]}\n","        \n","        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n","        self.relu1_1 = nn.ReLU(inplace=True)\n","        \n","        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n","        self.relu1_2 = nn.ReLU(inplace=True)\n","        self.pool1 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n","        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n","        self.relu2_1 = nn.ReLU(inplace=True)\n","        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n","        self.relu2_2 = nn.ReLU(inplace=True)\n","        self.pool2 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n","        \n","        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n","        self.relu3_1 = nn.ReLU(inplace=True)\n","        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n","        self.relu3_2 = nn.ReLU(inplace=True)\n","        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n","        self.relu3_3 = nn.ReLU(inplace=True)\n","        self.pool3 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n","        \n","        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n","        self.relu4_1 = nn.ReLU(inplace=True)\n","        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n","        self.relu4_2 = nn.ReLU(inplace=True)\n","        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n","        self.relu4_3 = nn.ReLU(inplace=True)\n","        self.pool4 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n","        \n","        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n","        self.relu5_1 = nn.ReLU(inplace=True)\n","        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n","        self.relu5_2 = nn.ReLU(inplace=True)\n","        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n","        self.relu5_3 = nn.ReLU(inplace=True)\n","        self.pool5 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n","        \n","        self.fc6 = nn.Linear(in_features=25088, out_features=4096, bias=True)\n","        self.relu6 = nn.ReLU(inplace=True)\n","        self.dropout6 = nn.Dropout(p=0.5)\n","        \n","        self.fc7 = nn.Linear(in_features=4096, out_features=4096, bias=True)\n","        self.relu7 = nn.ReLU(inplace=True)\n","        self.dropout7 = nn.Dropout(p=0.5)\n","        \n","        self.fc8 = nn.Linear(in_features=4096, out_features=2622, bias=True)\n","        self.softmax8 = nn.Softmax()\n","\n","        \n","    def forward(self, x0):\n","        x1 = self.conv1_1(x0)\n","        x2 = self.relu1_1(x1)\n","        x3 = self.conv1_2(x2)\n","        x4 = self.relu1_2(x3)\n","        x5 = self.pool1(x4)\n","        \n","        x6 = self.conv2_1(x5)\n","        x7 = self.relu2_1(x6)\n","        x8 = self.conv2_2(x7)\n","        x9 = self.relu2_2(x8)\n","        x10 = self.pool2(x9)\n","        \n","        x11 = self.conv3_1(x10)\n","        x12 = self.relu3_1(x11)\n","        x13 = self.conv3_2(x12)\n","        x14 = self.relu3_2(x13)\n","        x15 = self.conv3_3(x14)\n","        x16 = self.relu3_3(x15)\n","        x17 = self.pool3(x16)\n","        \n","        x18 = self.conv4_1(x17)\n","        x19 = self.relu4_1(x18)\n","        x20 = self.conv4_2(x19)\n","        x21 = self.relu4_2(x20)\n","        x22 = self.conv4_3(x21)\n","        x23 = self.relu4_3(x22)\n","        x24 = self.pool4(x23)\n","        \n","        x25 = self.conv5_1(x24)\n","        x26 = self.relu5_1(x25)\n","        x27 = self.conv5_2(x26)\n","        x28 = self.relu5_2(x27)\n","        x29 = self.conv5_3(x28)\n","        x30 = self.relu5_3(x29)\n","        x31_preflatten = self.pool5(x30)\n","        x31 = x31_preflatten.view(x31_preflatten.size(0), -1)\n","        \n","        x32 = self.fc6(x31)\n","        x33 = self.relu6(x32)\n","        x34 = self.dropout6(x33)\n","        \n","        x35 = self.fc7(x34)\n","        x36 = self.relu7(x35)\n","        x37 = self.dropout7(x36)\n","        \n","        x38 = self.fc8(x37)\n","        x39 = self.softmax8(x38)\n","        return x39\n","\n","    \n","def vgg_face_dag_init(weights_path=None, **kwargs):\n","    \"\"\"\n","    load imported model instance\n","\n","    Args:\n","        weights_path (str): If set, loads model weights from the given path\n","    \"\"\"\n","    model = vgg_face_dag()\n","    \n","    if weights_path:\n","        state_dict = torch.load(weights_path)\n","        model.load_state_dict(state_dict)\n","    return model\n","\n","\n","# vgg_face_dag_init('/kaggle/input/vgg-face-dag/vgg_face_dag.pth')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T14:57:02.200771Z","iopub.status.busy":"2024-04-13T14:57:02.200435Z","iopub.status.idle":"2024-04-13T14:57:33.013250Z","shell.execute_reply":"2024-04-13T14:57:33.012277Z","shell.execute_reply.started":"2024-04-13T14:57:02.200736Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgolovninlev\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["wandb version 0.16.6 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.2"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240413_145702-z06o7wge</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/golovninlev/emo_rec/runs/z06o7wge' target=\"_blank\">k04.13_14-57-02</a></strong> to <a href='https://wandb.ai/golovninlev/emo_rec' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/golovninlev/emo_rec' target=\"_blank\">https://wandb.ai/golovninlev/emo_rec</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/golovninlev/emo_rec/runs/z06o7wge' target=\"_blank\">https://wandb.ai/golovninlev/emo_rec/runs/z06o7wge</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/golovninlev/emo_rec/runs/z06o7wge?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7dd560522ec0>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["\n","if platform.system() == 'Windows':\n","    data_dir = '../data'\n","    models_dir = '../models'\n","else:\n","    data_dir = '/kaggle/input'\n","    experiment_name = 'k' + experiment_name\n","    wandb_local_dir = '/kaggle/working/'\n","    models_dir = '/kaggle/working/'\n","\n","\n","\n","\n","wandb.init(\n","    project=wandb_project_name, \n","    name=experiment_name, \n","    config={\n","        \"dataset\": folder_with_data,\n","        'data_transforms': data_transforms,\n","        \"architecture\": model_name,\n","        \n","        \"train_test_split\": 0.25,\n","        \n","        \"epochs\": num_epochs,\n","        \"batch_size\": batch_size,\n","        \n","        \"learning_rate\": lr,\n","        \"momentum\": momentum,\n","        \n","        'optim': optim_name,\n","    },\n","    dir = wandb_local_dir,\n","    notes=run_desc, # комментарии к запуску\n","    # group=\"Группа_экспериментов\", # несколько запусков под общим именем группы\n","    # tags=wandb_tags, # wandb_tags = [\"test\", \"try\"]\n","    # resume=\"идентификатор_вашего_запуска\" # возобновляет ранее завершенный запуск\n","    # entity=\"lev\", \n","    )\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T14:57:33.015439Z","iopub.status.busy":"2024-04-13T14:57:33.015058Z","iopub.status.idle":"2024-04-13T14:57:45.793446Z","shell.execute_reply":"2024-04-13T14:57:45.792453Z","shell.execute_reply.started":"2024-04-13T14:57:33.015404Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]},{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n","100%|██████████| 548M/548M [00:01<00:00, 327MB/s]  \n"]},{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["val_titile = 'val'\n","train_title = 'train'\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n","# if not torch.cuda.is_available(): \n","#     import torch_xla\n","#     import torch_xla.core.xla_model as xm\n","#     # for TPU\n","#     device = xm.xla_device()\n","#     torch.set_default_tensor_type('torch.FloatTensor')\n","print(device)\n","\n","\n","\n","\n","# !3 train and test split\n","# Создаем загрузчики данных для обучения и валидации\n","image_datasets = {x: datasets.ImageFolder(root=data_dir + '/' + x, transform=data_transforms[x]) for x in [folder_with_data]}\n","# dataloaders = {x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in [folder_with_data]}\n","\n","# Предположим, у вас есть dataset, который вы загружаете с помощью DataLoader\n","dataset = image_datasets[folder_with_data]\n","\n","# Определите размер тестового набора (например, 20% данных)\n","test_size = int(train_test_split * len(dataset))\n","\n","# Определите размер обучающего набора\n","train_size = len(dataset) - test_size\n","\n","# Разделите данные на обучающий и тестовый наборы, учитывая пропорции классов\n","train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n","\n","\n","dataloaders=dict({folder_with_data: DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4), \n","                  val_titile: DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)})\n","\n","# print(len(image_datasets[folder_with_data]), len(dataloaders[folder_with_data]), len(dataloaders[val_titile])) # 5710 429 143\n","# !3                                                     \n","\n","\n","\n","\n","\n","# !3 взвешивание дизбаланса классов + Loss\n","if folder_with_data == 'train-5000-190-35000-14000resol':\n","    weights = torch.tensor([0.0228, 0.7191, 0.1614, 0.0286, 0.0115, 0.0150, 0.0148, 0.0267]) \n","elif folder_with_data == 'train-5000-190-14000resol':\n","    weights = torch.tensor([0.0457, 0.4470, 0.2216, 0.1723, 0.0196, 0.0222, 0.0216, 0.0500])\n","elif folder_with_data == 'train-5000-190':\n","    weights = torch.tensor([0.0756, 0.3257, 0.1438, 0.1132, 0.0470, 0.1228, 0.0895, 0.0824])\n","elif folder_with_data == 'debug':\n","    weights = torch.tensor([0.1445, 0.3210, 0.2064, 0.2064, 0.0672, 0.0153, 0.0159, 0.0235])\n","elif folder_with_data == '35887-48x48-emotion-detection':\n","    weights = torch.tensor([0.0685, 0.9000, 0.6320, 0.0660, 0.0377, 0.0548, 0.0561, 0.0849]) # contempt = 0\n","elif folder_with_data == '35887-48x48-fer-2013':\n","    weights = torch.tensor([0.0692, 0.9000, 0.6260, 0.0679, 0.0387, 0.0556, 0.0568, 0.0858])\n","else:\n","    # Подсчет количества примеров каждого класса в обучающем наборе данных\n","    class_counts = torch.bincount(torch.cat([torch.tensor(target).view(-1) for _, target in dataloaders[folder_with_data].dataset], dim=0)) \n","    class_weights = 1 / class_counts.float()\n","    weights = class_weights / class_weights.sum()\n","    \n","    print(f'''elif folder_with_data == '{folder_with_data}':\n","    weights = torch.{weights}''')\n","\n","weights = weights.to(device)\n","\n","\n","criterion = nn.CrossEntropyLoss(weight=weights) \n","# !3                                                     \n","\n","\n","\n","\n","\n","# !4 model + optimizer\n","# Редактируем последний слой для задачи классификации на 8 классов\n","\n","if model_name == 'resnext101_32x32d__fb_wsl_ig1b_ft_in1k':\n","    model = timm.create_model('resnext101_32x32d.fb_wsl_ig1b_ft_in1k', pretrained=True)\n","    model.fc = nn.Linear(model.fc.in_features, num_classes)\n","\n","if model_name == 'seresnextaa101d_32x8d__sw_in12k_ft_in1k':\n","    model = timm.create_model('seresnextaa101d_32x8d.sw_in12k_ft_in1k', pretrained=True)\n","    model.fc = nn.Linear(model.fc.in_features, num_classes)\n","\n","if model_name == 'seresnet50__a1_in1k':\n","    model = timm.create_model('seresnet50.a1_in1k', pretrained=True)\n","    model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    \n","if model_name == 'seresnet50__ra2_in1k':\n","    model = timm.create_model('seresnet50.ra2_in1k', pretrained=True)\n","    model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    \n","if model_name == 'resnext101_32x4d.fb_swsl_ig1b_ft_in1k':\n","    model = timm.create_model('resnext101_32x4d.fb_swsl_ig1b_ft_in1k', pretrained=True)\n","    model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    \n","    \n","if model_name == 'seresnextaa101d_32x8d__sw_in12k_ft_in1k' \\\n","        or model_name == 'seresnet50__a1_in1k' \\\n","        or model_name == 'seresnet50__ra2_in1k':\n","    data_config = timm.data.resolve_model_data_config(model)\n","    transforms = timm.data.create_transform(**data_config, is_training=True)\n","    print(transforms)\n","    data_transforms[folder_with_data] = transforms\n","    \n","\n","if model_name == 'resnet50':\n","    model = models.resnet50(pretrained=True)\n","    model.fc = nn.Linear(model.fc.in_features, num_classes)\n","\n","if model_name == 'vgg19':\n","    model = models.vgg19(pretrained=True)\n","    model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n","\n","if model_name == 'vgg_face_dag':\n","    model = vgg_face_dag_init(path_to_pr_model)\n","    model.fc8 = nn.Linear(model.fc8.in_features, num_classes)\n","\n","\n","if path_to_pr_model != '':\n","    if path_to_pr_model[-4:] == '.pth' and model_name != 'vgg_face_dag':\n","        model.load_state_dict(torch.load(path_to_pr_model)) # обученная модель для классификации эмоций\n","\n","    if path_to_pr_model[-4:] == '.pkl':\n","        model = joblib.load(path_to_pr_model)\n","        if model_name == 'resnet50':\n","            model.fc = nn.Linear(model.fc.in_features, num_classes)\n","        if model_name == 'vgg19':\n","            model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n","\n","\n","\n","model.to(device)\n","if optim_name == 'SGD':\n","    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum) # SGD\n","    \n","if optim_name == 'Adam':\n","#     optimizer = optim.Adam(model.parameters(), lr=lr, betas=betas, eps=eps)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    \n","if optim_name == 'NAdam':\n","    optimizer = optim.NAdam(model.parameters(), lr=lr)\n","    \n","if optim_name == 'RAdam':\n","    optimizer = torch.optim.RAdam(model.parameters(), lr=lr)\n","    \n","if optim_name == 'Adamax':\n","    optimizer = torch.optim.Adamax(model.parameters(), lr=lr)\n","\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T14:57:45.795637Z","iopub.status.busy":"2024-04-13T14:57:45.795012Z","iopub.status.idle":"2024-04-13T16:27:03.168740Z","shell.execute_reply":"2024-04-13T16:27:03.167327Z","shell.execute_reply.started":"2024-04-13T14:57:45.795608Z"},"trusted":true},"outputs":[],"source":["for epoch in range(start_epoch, num_epochs):\n","    for phase in [folder_with_data, val_titile]:\n","        \n","        if phase == folder_with_data:\n","            model.train()\n","            phase_name = train_title\n","        else:\n","            model.eval()\n","            phase_name = val_titile\n","\n","        running_loss = 0.0\n","        corrects = 0\n","        all_labels = []\n","        all_preds = []\n","\n","\n","        for inputs, labels in dataloaders[phase]:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            with torch.set_grad_enabled(phase == folder_with_data):\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","\n","                if phase == folder_with_data:\n","                    loss.backward()\n","                    optimizer.step()\n","\n","            running_loss += loss.item() * inputs.size(0)\n","            _, preds = torch.max(outputs, 1)\n","            corrects += torch.sum(preds == labels.data)\n","            \n","            # Сохраняем истинные метки и предсказания для вычисления F1-Score\n","            all_labels.extend(labels.cpu().numpy())\n","            all_preds.extend(preds.cpu().numpy())\n","\n","\n","\n","        epoch_loss = running_loss / (len(dataloaders[phase]) * batch_size)\n","        # epoch_acc = (corrects.data.cpu() * 1.0) / (len(dataloaders[phase]) * batch_size)\n","        \n","        epoch_acc = accuracy_score(all_labels, all_preds)\n","        precision = precision_score(all_labels, all_preds, average='weighted')\n","        recall = recall_score(all_labels, all_preds, average='weighted')\n","        f1 = f1_score(all_labels, all_preds, average='weighted')\n","        # roc_auc = roc_auc_score(all_labels, all_preds, average='weighted', multi_class='ovr')\n","        \n","        # average_precision = average_precision_score(all_labels, all_preds, average='weighted')\n","        # cohen_kappa = cohen_kappa_score(all_labels, all_preds)\n","        # mc = matthews_corrcoef(all_labels, all_preds, average='weighted')\n","        # prfs = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n","        \n","        if phase == val_titile and epoch_acc > min_acc_to_save and epoch_loss < max_loss_to_save:\n","            torch.save(model.state_dict(), f'{models_dir}{experiment_name}_e{epoch}.pth')\n","#             import pickle\n","#             with open(f'{models_dir}{experiment_name}_e{epoch}.pkl', 'wb') as f:\n","#                 pickle.dump(model, f)\n","\n","        if epoch != num_epochs - 1:\n","            wandb.log({phase_name: {\n","                'loss': epoch_loss, 'acc': epoch_acc, 'precision': precision, 'recall': recall, 'f1': f1, \n","                # 'classification_report': wandb_table, \n","                }}, step=epoch)\n","        else:\n","#             class_rep_dict = classification_report(all_labels, all_preds, target_names=target_names,\n","#                 output_dict=True)\n","#             class_rep_dict.pop('accuracy')\n","#             class_rep_df = pd.DataFrame().from_dict(class_rep_dict).T.reset_index().round(2)\n","#             wandb_table = wandb.Table(data=class_rep_df, \n","#                         columns=[\"class\", \"precision\", \"recall\", \"f1-score\", \"support\"])\n","            \n","            conf_matrix = confusion_matrix(all_labels, all_preds)\n","            \n","            plt.figure()\n","            cm = confusion_matrix(y_true=all_labels, y_pred=all_preds)\n","            plot_cm = sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n","            plt.xlabel(\"Predicted\")\n","            plt.ylabel(\"True\")\n","            plt.title(f\"Confusion Matrix - {phase_name}\")\n","            \n","            \n","            wandb.log({phase_name: {\n","                # 'epoch': epoch, \n","                'loss': epoch_loss, 'acc': epoch_acc, 'precision': precision, 'recall': recall, 'f1': f1, \n","#                 'classification_report': wandb_table, \n","                'confusion_matrix': wandb.Image(plot_cm) # wandb.Image(img[0].numpy()*255)\n","                # 'average_precision': average_precision, 'cohen_kappa': cohen_kappa, 'matthews_corrcoef': mc,\n","                # 'precision_recall_fscore_support': prfs\n","                }}, step=epoch)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T16:27:03.171022Z","iopub.status.busy":"2024-04-13T16:27:03.170625Z","iopub.status.idle":"2024-04-13T16:27:05.927609Z","shell.execute_reply":"2024-04-13T16:27:05.926751Z","shell.execute_reply.started":"2024-04-13T16:27:03.170971Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.127 MB of 0.127 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">k04.13_14-57-02</strong> at: <a href='https://wandb.ai/golovninlev/emo_rec/runs/z06o7wge' target=\"_blank\">https://wandb.ai/golovninlev/emo_rec/runs/z06o7wge</a><br/>Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240413_145702-z06o7wge/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["wandb.finish()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T16:27:05.929135Z","iopub.status.busy":"2024-04-13T16:27:05.928770Z","iopub.status.idle":"2024-04-13T16:27:07.403223Z","shell.execute_reply":"2024-04-13T16:27:07.402176Z","shell.execute_reply.started":"2024-04-13T16:27:05.929091Z"},"trusted":true},"outputs":[],"source":["torch.save(model.state_dict(), f'{models_dir}{experiment_name}_e{epoch}.pth')"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T16:27:07.407061Z","iopub.status.busy":"2024-04-13T16:27:07.406636Z","iopub.status.idle":"2024-04-13T16:27:07.413968Z","shell.execute_reply":"2024-04-13T16:27:07.412891Z","shell.execute_reply.started":"2024-04-13T16:27:07.407024Z"},"trusted":true},"outputs":[{"data":{"text/plain":["1"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4287238,"sourceId":7377636,"sourceType":"datasetVersion"},{"datasetId":4411715,"sourceId":7578458,"sourceType":"datasetVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
